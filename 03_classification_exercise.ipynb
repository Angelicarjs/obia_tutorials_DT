{"cells":[{"cell_type":"markdown","metadata":{"id":"as9EVp4yGos-"},"source":["## **Introduction**"]},{"cell_type":"markdown","source":["To recap what we have covered so far: In the **first notebook**, we introduced different methods for segmenting an image, such as using thresholds or the Simple Linear Iterative Clustering (SLIC) approach. In the **second notebook**, we continued by exploring how to extract useful attributes from each segment, known as features. These features include metrics like mean intensity per segment, standard deviation, rectangularity, compactness, and more.\n","\n","In this notebook, we will bring everything together by using the features we've extracted to classify the segments we identified earlier. Specifically, our goal is to create a binary classifier that can determine whether each segment represents a tree in the image.\n","\n","<img src=\"https://github.com/mariarodriguezn/obia_tutorials/blob/main/assets/general_workflow.png?raw=true\" width=\"700\">\n","\n","\n"],"metadata":{"id":"CrwklsEnK0g0"}},{"cell_type":"markdown","metadata":{"id":"CNRBl8HeGotJ"},"source":["## **Setup**\n","Please run the cell below. After that, go to **Runtime -> Restart session**, and confirm. Once the session restarts, move on to the next cell **without running this one again**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YqS3sDBPGotL"},"outputs":[],"source":["# Check if running on Google Colab\n","if 'google.colab' in str(get_ipython()):\n","    import os\n","    repo_dir = \"obia_tutorials\"\n","    marker_file = os.path.join(repo_dir, \".setup_done\")\n","\n","    # Setup the environment only if it hasn't been done already\n","    if not os.path.exists(marker_file):\n","        # Clone the repository\n","        !git clone https://github.com/mariarodriguezn/obia_tutorials.git\n","\n","        # Install the required packages\n","        !pip install -r obia_tutorials/requirements.txt\n","\n","        # Create a marker file to avoid re-running the setup\n","        with open(marker_file, 'w') as f:\n","            f.write(\"Setup completed\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KQKMaGJTGotN"},"outputs":[],"source":["# imports\n","import matplotlib.colors as mcolors\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import time\n","import rasterio\n","\n","from scipy import stats\n","from skimage.color import label2rgb\n","from skimage.measure import regionprops, regionprops_table, perimeter\n","from skimage.segmentation import mark_boundaries, slic\n","from skimage.util import map_array\n","from sklearn.ensemble import RandomForestClassifier\n","from tqdm import tqdm\n"]},{"cell_type":"markdown","metadata":{"id":"wVbJdAZoGotP"},"source":["## **Image Segmentation**"]},{"cell_type":"markdown","metadata":{"id":"hpKtfiqJGotQ"},"source":["We will continue working with the same high-resolution image. As a reminder, this image contains four spectral bands: Red, Green, Blue, and Near-Infrared (NIR). Additionally, we will utilize the Normalized Difference Vegetation Index (NDVI) that was calculated from these bands.\n","\n","For the segmentation process we will again use SLIC algorithm with a compactness value of 0.3 and set the number of superpixels to 3000 as displayed below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OmqhF8e6GotR"},"outputs":[],"source":["# File path to the image\n","img_path = \"obia_tutorials/sample_data/ortho_subset_I.tif\"\n","\n","# Read the image and extract the bands\n","with rasterio.open(img_path) as src:\n","    # Extract red, green, blue, and NIR bands, normalize to [0, 1]\n","    bands = src.read([1, 2, 3, 4]).astype(float) / 255\n","    red, green, blue, nir = bands\n","\n","# Stack RGB into an array\n","rgb = np.stack([red, green, blue], axis=-1)\n","\n","# Calculate NDVI\n","ndvi = (nir - red) / (nir + red + 1e-6)  # small value added to avoid division by zero\n","\n","# Stack all bands into an array\n","image = np.stack([red, green, blue, nir], axis=-1)\n","\n","# Set the fixed compactness and number of superpixels\n","compactness = 0.3\n","n_segments = 3000\n","\n","# Generate segments using SLIC\n","segments = slic(image, n_segments=n_segments, compactness=compactness, start_label=1)\n","\n","# Create the figure with 3 subplots\n","fig, axs = plt.subplots(ncols=3, figsize=(15, 5), constrained_layout=True)\n","\n","# Display the original RGB image\n","axs[0].imshow(rgb)\n","axs[0].set_title(\"Original RGB\")\n","\n","# Display the RGB image with segmentation boundaries for 3000 segments\n","axs[1].imshow(mark_boundaries(rgb, segments, color=(1, 0, 0), mode=\"thick\"))\n","axs[1].set_title(f\"SLIC Segmentation Boundaries ({n_segments} segments)\")\n","\n","# Display the mean RGB per segment\n","axs[2].imshow(label2rgb(segments, rgb, kind='avg'))\n","axs[2].set_title(\"Mean RGB per Segment\")\n","\n","# Remove axis for all subplots\n","for ax in axs:\n","    ax.set_axis_off()\n","\n","# Display the combined figure\n","plt.show()"]},{"cell_type":"markdown","source":["## **Features Extraction**"],"metadata":{"id":"ZHGCi3dORthb"}},{"cell_type":"markdown","metadata":{"id":"QpA1WG9kGotw"},"source":["As explained in the second notebook, there are multiple features that can be extracted to characterize the segments using `region_props` function from the`skiimage` Python library. In this case, to train our classifier later, we will calculate the following features for each segment :\n","\n","* **Spectral Features** across different bands (Red, Green, Blue, NIR, and NDVI)\n","  * Intensity Mean\n","  * Standard Deviation\n","\n","* **Shape Features**\n","  * Rectangularity\n","  * Compactness\n","  * Solidity\n","\n","* **Textural Features**\n","  * Entropy of NDVI\n"]},{"cell_type":"code","source":["# Stack all bands into an array\n","full_stack_arr = np.stack([red, green, blue, nir, ndvi], axis=-1)\n","\n","# Define custom features\n","def std(regionmask, intensity_img):\n","    vals = intensity_img[regionmask]\n","    std = np.std(vals)\n","    return std\n","\n","def rectangularity(regionmask):\n","    return regionmask.sum()/regionmask.size\n","\n","def compactness(regionmask):\n","    return 4*np.pi*regionmask.sum()/(perimeter(regionmask)**2 + 1e-6)  # small value added to avoid division by zero\n","\n","def entropy_ndvi(regionmask, intensity_img):\n","    vals = intensity_img[regionmask]\n","    arr = stats.relfreq(vals, 100, defaultreallimits=(-1,1))[0]\n","    return stats.entropy(arr)\n","\n","# Calculate set of various features for each segment\n","def calc_all_feats(seg_arr, img_arr):\n","    spectral_feats = pd.DataFrame(\n","        regionprops_table(\n","            label_image = seg_arr,\n","            intensity_image = img_arr,\n","            properties = [\"label\", \"intensity_mean\"],\n","            extra_properties=(std,)\n","        )\n","    )\n","    shape_feats = pd.DataFrame(\n","        regionprops_table(\n","            label_image = seg_arr,\n","            properties = [\"solidity\"],\n","            extra_properties=(rectangularity, compactness)\n","        )\n","    )\n","\n","    textural_feats = pd.DataFrame(\n","        regionprops_table(\n","            label_image = seg_arr,\n","            intensity_image =img_arr[:, :, -1],  # Use only the NDVI band\n","            properties = [],\n","            extra_properties=(entropy_ndvi,)\n","        )\n","    )\n","\n","    all_feats = pd.concat([spectral_feats, shape_feats, textural_feats], axis=1)\n","\n","    return all_feats\n","\n","all_feats_df = calc_all_feats(segments, full_stack_arr)\n","display(all_feats_df)"],"metadata":{"id":"UEvwE7IhhTxk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Image Classification**"],"metadata":{"id":"7s8A5ZRXtj9A"}},{"cell_type":"markdown","source":["\n","Now, using the 14 features calculated above, we we will create a binary classifier to identify the trees in the image using Random Forest Algorithm.\n","\n","Random Forest is a machine learning algorithm that works by creating multiple decision trees during training. Each tree makes a prediction, and the Random Forest aggregates these predictions to determine the final output. This method helps improve accuracy and reduces the risk of overfitting.\n","\n","The key stages of this process are:\n","\n","<img src=\"https://github.com/mariarodriguezn/obia_tutorials/blob/main/assets/image_classification_workflow.png?raw=true\" width=\"800\">\n","\n","\n","\n","\n"],"metadata":{"id":"vOZCdyLjSNqG"}},{"cell_type":"markdown","source":["### **Label Image Segments**"],"metadata":{"id":"MqmhX6MUzLpW"}},{"cell_type":"markdown","source":["For the purpose of this exercise, the sample segments labeled as trees and non-trees are already provided in the following cell. Additionally, a new column called **`class`** is added to the existing DataFrame `all_feats_df`, which contains all the segments along with their corresponding features. This new column indicates the class, with **1 for trees, 0 for non-trees, and -1 for unlabeled segments**."],"metadata":{"id":"pKQk-dkmzW1v"}},{"cell_type":"code","source":["# Samples for both classes: trees and non-trees\n","trees_idxs = [\n","    1204, 1186, 1134, 1229, 1329, 1201, 1140, 1218, 1351, 962, 830, 886, 926, 1373, 1405, 1517, 1547, 1461, 1526, 1406, 1358, 1460, 1556, 735, 833, 838, 802, 513, 616, 565, 438, 258, 375, 334, 284, 204, 250, 155, 183, 241, 344, 359, 412, 328, 424, 506, 468, 554, 459, 579, 560, 769, 890, 791, 798, 811, 771, 846, 1076, 784, 919, 915, 133, 246, 356, 405, 425, 474, 166, 158, 34, 200, 209, 245, 55, 2003, 1984, 1910, 1813, 1873, 1923, 2296, 2220, 2143, 2113, 2153, 2319, 2275, 2284, 2422, 2451, 2398, 2460, 2455, 2208, 2218, 2199, 2036, 1986, 1924\n","]\n","\n","non_trees_idxs = [\n","    2647, 2653, 2717, 2784, 2110, 2061, 2122, 1988, 1943, 2018, 1861, 1896, 1909, 1900, 1829, 1758, 1719, 1703, 1794, 1812, 1749, 1581, 1580, 1522, 1451, 1438, 1479, 1408, 1511, 1631, 1869, 1811, 1978, 2104, 2187, 2299, 2400, 2732, 2642, 2567, 2348, 2369, 2401, 2281, 1850, 1117, 197, 1002, 872, 216, 432, 418, 357, 382, 419, 336, 295, 351, 460, 644, 708, 800, 902, 1005, 1063, 174, 824, 831, 780, 1291, 1412, 1537, 1497, 1570, 1605, 1491, 1595, 1652, 1768, 1807, 1887, 1898, 1968, 1573, 1540, 1536, 1439, 1388, 1694, 2640\n","]\n","\n","# Initialize the 'class' column in all_feats_df to -1 (for unlabeled segments)\n","all_feats_df['class'] = -1\n","\n","# Update the 'class' column for tree segments (label 1)\n","all_feats_df.loc[all_feats_df['label'].isin(trees_idxs), 'class'] = 1\n","\n","# Update the 'class' column for non-tree segments (label 0)\n","all_feats_df.loc[all_feats_df['label'].isin(non_trees_idxs), 'class'] = 0\n","\n","# Display the updated DataFrame\n","all_feats_df = all_feats_df.rename(columns={'label': 'indx'})\n","display(all_feats_df)\n"],"metadata":{"id":"zCkHXHLH3Wlf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To better understand the location of the given labeled samples, they can be displayed displayed based on the class."],"metadata":{"id":"vS8Iixzv6wp3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LY0E695xGotx"},"outputs":[],"source":["# Map the labeled segments onto the original segmented image\n","mapped_samples = map_array(\n","    segments,\n","    np.array(all_feats_df[\"indx\"]),\n","    np.array(all_feats_df[\"class\"])\n",")\n","\n","# Define the colors:\n","# [-1] = undefined segments (grey), [0] = non-trees (dark blue), [1] = trees (green)\n","background_c, others_c, trees_c = '#7f8282', '#00008b', '#00FF00'\n","binary_cmap = mcolors.ListedColormap([background_c, others_c, trees_c])\n","\n","# Display the original image with segment boundaries\n","plt.figure(figsize=(12, 8))\n","plt.imshow(mark_boundaries(rgb, segments, (1, 0, 0), mode=\"outer\"))\n","\n","# Overlay the labeled segments with the defined colors\n","plt.imshow(mapped_samples, cmap=binary_cmap, alpha=0.5)\n","\n","# Turn off the axis for a cleaner look\n","plt.axis(\"off\")\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"markdown","source":["#####**Task 1**\n","Taking into account the image above and the knowledge you have acquired in this course, answer: **What criteria are considered when selecting or creating samples for training in a classification model?** Hint: *An example of a criteria that could be mentioned and discuss is **Spatial Distribution**.*"],"metadata":{"id":"-HZ8vQBlWeUg"}},{"cell_type":"markdown","source":["**You answer goes here:**"],"metadata":{"id":"_RfvdyTaWfto"}},{"cell_type":"markdown","source":["### **Train Random Forest Classifier**"],"metadata":{"id":"Cq_oxFsr_UUl"}},{"cell_type":"markdown","source":["In this stage, we start by selecting only the image segments that we have labeled as \"tree\" or \"non-tree,\" ignoring any segments without labels. We then split the data into two parts: the features columns (which are the characteristics we measured from each segment) and the class column (which tells us if the segment is a tree or not).\n","\n","Next, we train a Random Forest model, which as mentioned is a type of machine learning method that creates many decision trees to improve accuracy. We set it to create 100 trees and use the Out-of-Bag (OOB) score to check how well the model might work on new, unseen data. The OOB score is a built-in way to test the model's accuracy without needing a separate test set, making it a practical way to assess how well the model is likely to perform."],"metadata":{"id":"qxt5E2WPIDRn"}},{"cell_type":"markdown","source":["#####**Task 2**\n","Complete the code to train the Random Forest Classifier (clf) using the following parameters:\n","\n","1.   n_estimators = 100\n","2.   obb_score = True\n","3.   random_state = 42\n","\n","*Hint: You can use the* \"**RandomForestClassifier**\" *method.*\n"],"metadata":{"id":"XuZw4Ip-alrZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bYyJHUnVGot0"},"outputs":[],"source":["# Filter out the labeled samples\n","labeled_segments_df = all_feats_df[all_feats_df['class'] != -1]\n","\n","# Separate features (X) and class label (y)\n","X = labeled_segments_df.drop(columns=['indx', 'class'])\n","y = labeled_segments_df['class']\n","\n","#### Your code here to Train the Random Forest Classifier ####\n","clf =\n","\n","clf.fit(X, y)\n","\n","# Evaluate the model using OOB score\n","print(\"OOB Score:\", clf.oob_score_)"]},{"cell_type":"markdown","source":["#####**Task 3**\n","Based on the obtained OOB score, answer:\n","\n","\n","*  **What does the OOB score indicate about the performance of your model?** Consider what the score reveals about the modelâ€™s ability to generalize to new data and how well it distinguishes between 'tree' and 'non-tree' segments.\n","\n","\n"],"metadata":{"id":"yRCH959dchoI"}},{"cell_type":"markdown","source":["**You answer goes here:**"],"metadata":{"id":"To1QIZPmdCrS"}},{"cell_type":"markdown","source":["### **Test the Classifier**"],"metadata":{"id":"KZajgTE5_5CA"}},{"cell_type":"markdown","source":["Now, to further evaluate the classifier, we apply the trained classifier to the **unlabeled** segments. This step is fundamental to determine how well the classifier can generalize beyond the labeled training data and make accurate predictions on new, unseen segments."],"metadata":{"id":"5FXqjz7mQY9k"}},{"cell_type":"code","source":["# Filter out the unlabeled segments\n","unlabeled_segments_df = all_feats_df[all_feats_df['class'] == -1]\n","\n","# Separate features (X) for the unlabeled segments\n","X = unlabeled_segments_df.drop(columns=['indx', 'class'])\n","\n","# Predict the class labels for the unlabeled segments\n","predictions = clf.predict(X)\n","\n","# Update the original dataframe with the predictions\n","all_feats_df.loc[all_feats_df['class'] == -1, 'class'] = predictions"],"metadata":{"id":"CseZIzwVMfsa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Interpret the results**"],"metadata":{"id":"LYVrdNkI_74H"}},{"cell_type":"markdown","source":["Finally, to actually analyze the resulting prediction on the unlabeled segments, below the results are plot."],"metadata":{"id":"9U0fwdniUj-c"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xyEgfglqGot1"},"outputs":[],"source":["#  Map the predictions onto the original segmented image\n","mapped_predictions = map_array(\n","    segments,\n","    np.array(all_feats_df[\"indx\"]),\n","    np.array(all_feats_df[\"class\"]))\n","\n","# Create the figure with 3 subplots\n","fig, axs = plt.subplots(ncols=3, figsize=(15, 5), constrained_layout=True)\n","\n","# Display the original RGB image\n","axs[0].imshow(rgb)\n","axs[0].set_title(\"Original RGB\")\n","\n","# Display the RGB image with segmentation boundaries for 3000 segments\n","axs[1].imshow(mark_boundaries(rgb, segments, color=(1, 0, 0), mode=\"thick\"))\n","axs[1].set_title(f\"SLIC Segmentation Boundaries\")\n","\n","# Display the prediction result\n","axs[2].imshow(mapped_predictions, cmap=binary_cmap, interpolation=\"nearest\")\n","axs[2].set_title(\"Random Forest classifier prediction\")\n","\n","# Remove axis for all subplots\n","for ax in axs:\n","    ax.set_axis_off()\n","\n","# Display the combined figure\n","plt.show()"]},{"cell_type":"markdown","source":["#####**Task 4**\n","Considering the classification results shown in the image above, discuss the following:\n","\n","1.   Was the resulting classification good enough?\n","2.   Could the limited number of training samples have affected the model's performance?\n","3. What improvements could be made to enhance the accuracy of the classification?"],"metadata":{"id":"bvSAQQehfyBY"}},{"cell_type":"markdown","source":["**You answer goes here:**"],"metadata":{"id":"hTRegmVUf6NK"}},{"cell_type":"markdown","metadata":{"id":"ehLmvQysGot2"},"source":["## **Conclusion & Further thoughts**"]},{"cell_type":"markdown","metadata":{"id":"YELart03Got2"},"source":["This tutorial provided a simple example of how an image can be segmented, its features extracted, and then classified using a random forest algorithm.\n","\n","Future directions for exploration could include scaling up these experiments by applying machine learning models like random forests to larger and more diverse datasets. A key area of interest might be identifying the smallest, most computationally efficient set of features that still achieve high accuracy for specific tasks. This is particularly relevant in the context of the growing prominence of deep learning models. Although training deep learning models requires large amounts of data and can be time-consuming, their inference time (meaning the time it takes to apply the model to new, unseen data) is typically very fast."]}],"metadata":{"kernelspec":{"display_name":"obia_tutorials","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.1"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"03ca474701badda27714bc2f8463045a15615715ede98ee8a907c42ed944622a"}},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}